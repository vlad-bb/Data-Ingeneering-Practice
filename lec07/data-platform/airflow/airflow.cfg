[providers]
# Disable Google provider packages
disable_package_providers = ['google', 'google_cloud', 'apache-airflow-providers-google']

# Limit providers to only those needed
enabled_providers = ['http', 'postgres', 'common.sql', 'docker']

[core]
# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use
executor = LocalExecutor

# Secret key to save connection passwords in the db
fernet_key = $(AIRFLOW_FERNET_KEY)

# Whether to load the examples
load_examples = False

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to enable XCom pickling
enable_xcom_pickling = False

# Default timezone
default_timezone = utc

[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://$(POSTGRES_AIRFLOW_USER):$(POSTGRES_AIRFLOW_PASSWORD)@$(POSTGRES_AIRFLOW_HOST)/$(POSTGRES_AIRFLOW_DB)

# Whether to apply schema migrations on database initialization
# This will create necessary tables on initialization
sql_alchemy_pool_recycle = 1800

[scheduler]
# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 10

[webserver]
# The host interface the webserver should listen on
web_server_host = 0.0.0.0

# The port the webserver should listen on
web_server_port = 8080

# Secret key used to run your flask app
secret_key = ${AIRFLOW_SECRET_KEY}

# Number of workers to run the Gunicorn web server
workers = 4

# Access log format
access_logformat = %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s %%(L)s %%(f)s

# The timeout for old workers to handle requests before getting killed
worker_timeout = 120

# Expose the configuration file in the web server
expose_config = False

# Default DAG view
dag_default_view = tree

# Default DAG orientation
dag_orientation = LR

# Wether to serve the web UI in async mode
async_dagbag_loader = True

# How many processes to use to preload the DAGs
workers_refresh_batch_size = 1

[smtp]
# If you want airflow to send emails, uncomment and configure these settings
# smtp_host = localhost
# smtp_starttls = True
# smtp_ssl = False
# smtp_user = airflow
# smtp_password = airflow
# smtp_port = 25
# smtp_mail_from = airflow@example.com

[dbt]
# dbt project directory
project_dir = /opt/airflow/dags/dbt/my_dbt_project
profiles_dir = /opt/airflow/dags/dbt
target_database = $(ANALYTICS_DB)